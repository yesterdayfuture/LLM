在Hugging Face Transformers库中，微调后的模型保存与加载方式因微调方法（如常规微调或参数高效微调）而异。
### 一、常规微调模型的保存与加载
 #### 1、 保存完整模型
- 使用 save_pretrained() 方法可将整个模型（包含权重、配置、分词器）保存到指定目录：
	保存模型、分词器、配置
```python
model.save_pretrained("./my_finetuned_model")
tokenizer.save_pretrained("./my_finetuned_model")
```
 - 生成以下文件：
	pytorch_model.bin（或 tf_model.h5）：模型权重
	
	config.json：模型结构配置
	
	tokenizer_config.json：分词器配置
#### 2、加载模型

- 通过 from_pretrained() 加载已保存的完整模型：
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("./my_finetuned_model")
tokenizer = AutoTokenizer.from_pretrained("./my_finetuned_model")
```
### 二、参数高效微调（如LoRA）的保存与加载
#### 1、仅保存适配器权重
使用LoRA等参数高效方法时，只需保存增量权重（通常几十MB）：
- 保存LoRA适配器

```python
model.save_pretrained("./lora_adapter")
```

- 生成文件：
	adapter_model.bin（或 .safetensors）：适配器权重
	adapter_config.json：适配器配置（含基础模型路径）
#### 2、加载适配器

加载适配器，与原模型合并：

```python
from transformers import AutoModelForCausalLM
from peft import PeftModel, PeftConfig

# 加载基础模型
base_model = AutoModelForCausalLM.from_pretrained("./base_model")  

# 加载PEFT适配器
peft_config = PeftConfig.from_pretrained("./peft_adapter")
# 将adapter_model.safetensors中的低秩矩阵（如LoRA的A/B矩阵）加载到基础模型的指定位置（如q_proj、v_proj）
'''
PEFT的核心思想是通过冻结基础模型（如Llama-2、Mistral等）的原始参数，仅训练适配器层（如LoRA的低秩矩阵）。保存模型时：

​基础模型权重：保持原始精度（如FP32）或量化状态（如4-bit NF4）
​适配器参数：以独立文件存储（adapter_model.safetensors）
​配置元数据：记录适配器类型、注入位置等（adapter_config.json）
'''
peft_model = PeftModel.from_pretrained(base_model, "./peft_adapter")

```
#### 3、合并权重

- 若需独立部署，可将LoRA权重合并到原模型中：

```python
merged_model = model.merge_and_unload()
merged_model.save_pretrained("./merged_model")  # 保存完整模型（数GB）
```
- 加载完整模型

```python
# 直接加载完整模型
full_model = AutoModelForCausalLM.from_pretrained("./full_model") 
```

### 三、使用Trainer API的自动保存

通过 TrainingArguments 设置自动保存策略：

```python
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="./results",
    save_strategy="epoch",  # 按epoch保存
    load_best_model_at_end=True  # 训练结束后加载最佳模型
)
```

训练完成后，最佳模型会保存在 output_dir 中

```python
trainer = Trainer(model, args, ...)
trainer.train()
```

### 四、unsloth 微调后的模型 保存
基于 Unsloth 的微调模型保存具有高度灵活性
#### 1、LoRA 适配器轻量级保存

```python
model.save_pretrained("./lora_model")  # 仅保存适配器（约100MB）
tokenizer.save_pretrained("./lora_model")
```

**特点：**

- 仅存储微调增量参数，适合迭代开发阶段  

- 需配合原始基础模型加载使用  

- 文件结构：adapter_model.bin + adapter_config.json
#### 2、全量模型合并保存

- 合并为FP16精度（生产环境推荐）

```python
model.save_pretrained_merged(
    output_dir="merged_16bit",
    tokenizer=tokenizer,
    save_method="merged_16bit"
)
```

- 4位量化版（显存敏感场景）

```python
model.save_pretrained_merged(
    output_dir="merged_4bit",
    tokenizer=tokenizer,
    save_method="merged_4bit"
)
```


#### 3、GGUF 高效推理格式导出

- 标准Q8_0量化（平衡型）

```python
model.save_pretrained_gguf("gguf_model", tokenizer)
```

- 极致压缩Q4_K_M（移动端适用）

```python
model.save_pretrained_gguf(
    "gguf_model",
    tokenizer,
    quantization_method="q4_k_m"
)
```
#### 4、多格式混合保存

- 适配器 + 合并模型 + GGUF

```python
model.save_pretrained("lora_model")  # 适配器
model.save_pretrained_merged(...)     # 16bit合并
model.save_pretrained_gguf(...)      # GGUF格式
```


